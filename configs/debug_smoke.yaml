model:
  backend: dummy
  vocab_size: 256
  d_model: 128
  dropout: 0.0
  segment_masking: true

# Offline debug source: still exercises tokenize+pack.
data:
  backend: local_text
  repeat: true
  packing_mode: bin
  packing_buffer_docs: 32
  packing_max_docs_per_bin: null
  grain_prefetch: 2
  mask_boundary_loss: true
  train_on_eos: true
  max_eval_samples: 1000
  local_text: |
    Hello from chomp. This is a local_text debug stream.
    It exists so tests and smoke runs don't depend on network.
  tokenizer:
    kind: byte
    byte_offset: 0
    add_bos: false
    add_eos: false

train:
  seed: 0
  steps: 10
  batch_size: 2
  seq_len: 128
  grad_accum: 2
  jit: true
  deterministic: true
  allow_cpu: true
  log_every: 1
  eval_every: 0
  profile: false
  profile_dir: null

optim:
  lr: 3.0e-4
  weight_decay: 0.01
  grad_clip_norm: 1.0
  warmup_steps: 2
  total_steps: 10

checkpoint:
  enabled: true
  root_dir: null
  save_every: 5
  max_to_keep: 3
  max_save_checkpoints: 3
  async_save: false

logging:
  project: chomp
  run_dir: null
  metrics_file: metrics.jsonl
  level: INFO
  console_every: 1
  console_use_rich: true
  log_file: train.log

debug:
  nan_check: true
  check_device_every: 1
